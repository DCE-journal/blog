[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the blog for Data-Centric Engineering (cambridge.org/dce), an open-access journal at the interface of engineering and data science.\n\nEditors in chief\nEleni Chatzi (ETH Zürich), Mark Girolami (University of Cambridge & The Alan Turing Institute), Kenichi Soga (University of California).\n\n\nExecutive editors\nStéphane Bordas (University of Luxembourg), Roger Ghanem (University of Southern California), Detlef Hohl (Shell), Visakan Kadirkamanathan (University of Sheffield), Youssef Marzouk (Massachusetts Institute of Technology), Omar Matar (Imperial College London), Sumeetpal Singh (University of Cambridge), Kenichi Soga (University of California Berkeley), David White (University of Southampton), Philip J. Withers (University of Manchester).\n\n\n\nBlog edited by Lawrence Bull (University of Glasgow).\n\nThe views expressed in this blog are those of the listed author, they do not represent the formal view of the DCE team."
  },
  {
    "objectID": "resources/DCE_blog_blank.html",
    "href": "resources/DCE_blog_blank.html",
    "title": "Title",
    "section": "",
    "text": "Competing Interest:\nKeywords:\n\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X. Here are instructions for submitting an article to the journal."
  },
  {
    "objectID": "posts/09_world_avatar/world_avatar.html",
    "href": "posts/09_world_avatar/world_avatar.html",
    "title": "The World Avatar: Revolutionising Human-in-the-Loop",
    "section": "",
    "text": "This blog offers informed opinions and perspectives relating to nascent technologies in data-centric engineering. Prof. Markus Kraft discusses work towards human-in-the-loop modelling at scale, conducted via the Centre for Advanced Research and Education in Singapore.\n\nHuman-in-the-loop models represent the interaction between humans and automated systems, where human input is needed to operate, train, or improve a system. In these models, humans are integrated into the workflow of an automated process as both the goal-setter and goal-executor.\n\n\n\nPhoto by Aleksandr Popov on Unsplash\n\n\nAs a simple example, the task of baking a cake can be described as a human-in-the-loop system, where the human asks the why (the goal) and the how (the method). The why is broken into subgoals in the recipe, which the human can read and understand, such as: beat the eggs, weigh the flour, and turn on the oven. The human then carries out the how of these tasks using the correct tools: whisks, weighing scales, oven, etc.\n\n\n\nA conventional human-in-the-loop system\n\n\nMore complex human-in-the-loop systems are prevalent across various industries, integrating human intelligence and decision-making with automated processes. From drug discovery and material science to robotic surgery and autonomous vehicles, human-in-the-loop systems harness the strengths of both humans and machines across diverse applications.\nWhat these applications lack is a consistent knowledge model. The data lacks context. What is understandable to a human is incomprehensible to a machine. For example, the word “bank” can refer to a financial institution or the side of a river, and without semantic context, a machine cannot determine the correct meaning based on its usage. This is a fundamental barrier to the development of intelligent automation and AI systems.\nThe World Avatar project introduces a common framework for data representation, ensuring that information is interpretable, interconnected, and meaningful across different contexts and applications. This not only enhances the ability of machines to understand and work with human-generated data but also empowers humans to interact with machines in more intuitive and productive ways.\nThe project uses ontologies to represent any data or information in a logical knowledge graph. It uses semantic web technologies distributed over the internet to interact with any software and hardware. The entire ecosystem is dynamic, so new information is automatically cascaded through the entire knowledge graph. This revolutionary approach can autonomously determine the how for any given why.\nSo, what does this mean? Essentially, the World Avatar can derive information and knows how to solve problems. Given enough information, such a system would know exactly how to create new drugs, build a smart city, or reduce carbon emissions across entire industries. It could be applied to the world’s most complex and interconnected problems.\n\n\n\nThe World Avatar-enabled human-in-the-loop system\n\n\nThe World Avatar does not eliminate the human element; instead, it enhances and refines our role in the process. Humans remain integral, continuing to set overarching goals and participating actively in the execution of tasks. However, how we engage with the system and contribute to the workflow undergoes a transformative change.\nIn the World Avatar-enabled human-in-the-loop model, humans retain their position as goal-setters. We define the why, articulating our objectives and aspirations. This is a critical function, as it ensures that the system’s actions are aligned with our values, preferences, and desired outcomes.\nOnce the goals are set, the World Avatar takes the reins, navigating the complexities of the how. It draws from a wealth of interconnected knowledge (from quantum calculations to nationwide simulations) and breaks down the overarching objectives into sub-goals, creating a roadmap for execution.\nWhat’s more, findings from the World Avatar can be used to control real-world processes. Our recent case study used cutting-edge laboratory equipment to control automated experiments via the results of simulations populated by data from and executed within, the World Avatar.\nThe World Avatar is ready to redefine human-in-the-loop systems, ushering in a new era of intelligent automation and human-machine collaboration. By seamlessly integrating vast repositories of knowledge through dynamic knowledge graphs and ontologies, the World Avatar transcends the traditional boundaries of automated systems, offering unparalleled ways to solve problems and make decisions.\nTo learn more about the World Avatar, watch the associated DCE webinar on YouTube.\n\nCompeting Interest: Prof. Markus Kraft is a Fellow of Churchill College Cambridge and Professor in the Department of Chemical Engineering and Biotechnology. He is the director of CARES ltd., the Singapore-Cambridge CREATE Research Centre.\nKeywords: Dynamic Knowledge Graphs; Digital Twin Technology; Semantic Web; System of Systems; Automated Decision-Making; Human-AI Collaboration\n\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X. Here are instructions for submitting an article to the journal."
  },
  {
    "objectID": "posts/06_characterising_extremes/characterising_extremes.html",
    "href": "posts/06_characterising_extremes/characterising_extremes.html",
    "title": "Characterising Extreme Environments",
    "section": "",
    "text": "This blog offers informed opinions and perspectives relating to nascent technologies in data-centric engineering. Matthew Jones (Shell) and Philip Jonathan (Shell, Lancaster University) introduce ideas from extreme value statistics and discuss practical challenges related to their publications in weather modelling and oceanography.\n\nQuantifying the probability that a system (like a coastal defence, or a stock market) will fail, over some period of operation, often critically depends on models of extreme (or rare) events, to which the system is exposed. If we understand the extremes, we can design the system with sufficient reliability. This post introduces extreme value modelling and outlines some practical challenges. We also provide links to recent contributions by the authors.\nA typical prediction problem in data science involves trying to estimate what we expect to happen in situations we haven’t yet observed, and maybe quantify the uncertainty of a prediction. Usually, we are trying, in some sense, to interpolate between situations we think we understand (maybe because we have observations) and intermediate situations for which predictions are needed. Modelling extremes presents a different kind of challenge. Now we might want to predict the magnitudes of very rare events, which are likely never to have been seen before. In this sense, we need to extrapolate. The central limit theorem tells us that the distribution of parameter estimates in an empirical predictive model often tends to a Gaussian distribution as the number of observations increases. This allows us to quantify the uncertainty of predictions. When modelling extremes, there are similar asymptotic theorems describing the distribution of maxima of random variables (over a large spatial or temporal interval, say) and the distribution of exceedances of a high threshold. These theorems provide a basis for fitting the tails of distributions of individual system output variables.\n\n\n\nSatellite image of storms and hurricanes in the tropical Atlantic (courtesy of NOAA)\n\n\nIn practice, estimating extremes is complicated by many effects. We sometimes want to describe the tails of multivariate distributions, where some variables might be extreme, and other variables not. We might be interested in modelling extremes in space, or in time. In all these cases, specifying a model for the joint behaviour of random variables with appropriate asymptotic characteristics is important. Extremes from our physical environment (like atmospheric pressure fields, ocean waves, rainfall, river levels, earthquakes and heatwaves) typically depend on covariates: so the tail of the wind speed distribution might depend on the characteristics of the pressure field generating it (see the Figure) and the relative position of the observation location. This means we have to allow model parameters to vary with such covariates.\nThere is an active community of “extremist” (!) researchers in statistics and applied disciplines, both in academia and industry. Researchers at Shell have developed efficient computational methods to estimate non-stationary marginal, conditional, spatial and time-series extremes, often facilitated by long-standing collaborative projects with universities such as Lancaster. The holy grail in risk assessment is to be able to characterise and quantify all sources of uncertainty, including data measured with error, and approximate or misspecified models, so that system design and operational decisions are made as well as possible.\n\nCompeting Interest: Matthew Jones is a Statistician at Shell; Philip Jonathan is Chief Statistician at Shell, and Chair of Environmental Statistics and Data Science at Lancaster University.\nKeywords: Oceanography; Extreme Value Statistics; Sustainability\n\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X. Here are instructions for submitting an article to the journal."
  },
  {
    "objectID": "posts/10_EI_chatzi/EI_chatzi.html",
    "href": "posts/10_EI_chatzi/EI_chatzi.html",
    "title": "Editor’s Insights: Data-Centric Engineering of Monitored Systems",
    "section": "",
    "text": "This blog offers informed opinions and perspectives relating to nascent technologies in data-centric engineering. The DCE journal co-editor-in-chief Prof. Eleni Chatzi (Chair of Structural Mechanics at ETH Zürich) discusses emerging themes of physics-informed machine learning, digital twins, and systems thinking.\n\n\n\n\nPhoto by Michael Dziedzic on Unsplash\n\n\nOver past 17 years, and in my current capacity as Chair of Structural Mechanics & Monitoring at ETH Zurich, I have been working on the coupling of data with models that are driven from first principles (physics, mechanics, dynamics) to efficiently track and simulate monitored systems. Today, data seems omnipresent and abundant, as it is possible to extract from diverse products, processes, and structures. In the domain of Structural Health Monitoring (SHM), data of various forms can be obtained from sensors that can be deployed (via attachment, mobile sensing or remotely) on structures or mechanical systems. Contrary to popular belief, such systems are, in fact, like living, breathing organisms, which experience motion, and injury — they are exposed to adverse loads and natural hazards, age and deteriorate. Damage to such systems, which form our critical infrastructure and transportation networks, implies economic loss and impairment of our mobility and societal structures. In isolated cases, structures that fail can cost human lives. With a view to safeguard and protect structures and engineered systems, SHM essentially proposes a chain-like architecture, with the first link pertaining to data collection. However, data alone is not enough. Engineering models offer the opportunity to complement our observations by seeding in positive inductive biases, which allow us to filter raw data into interpretable and explainable information. Such models typically rely on principles of mechanics and dynamics. Oftentimes, these even come in the form of empirical laws.\n\nThe fusion of data with physics-based models is often referred to as hybrid modelling, and it allows us to create virtualisations or Digital Twins of engineered systems. Hybrid models often rely on Machine Learning (ML) schemes to model the portion of the physics that remains unknown, in which case, the term physics-enhanced model can be used. The balancing of physics involved with the amount of data available prescribes, in essence, a spectrum of possible schemes, which can rely more or less on the underlying physics-based assumptions (covering a white-to-grey-to-black box range). The information generated from such a merger can then be fed into the third link of the SHM chain, which corresponds to the task of diagnosis and prognosis, concerning the performance of these systems. In the very final link, the intention is to capitalise on the knowledge gained from the finite set of monitored instances, to support decisions and management at the systems level. For instance, the set of bridges and tunnels across a roadway network, the wind turbines within a farm, or the portfolio of offshore farms in the North Sea.\nIn a way, this chain is a natural parallel of the umbrella concept that is data science, since, at its core lie the concepts of data, computing, and algorithms to enable cognitive tasks. I am thrilled to have joined the DCE team as a co-editor-in-chief. I share its mission to explore the benefits of data science to improve the reliability, resilience, safety, and usability of engineered systems, and to promote autonomy and self-awareness in our engineered world.\n\nCompeting Interest: Eleni Chatzi is currently an Associate Professor, and the Chair of Structural Mechanics, at the Institute of Structural Engineering, of the Department of Civil, Environmental and Geomatic Engineering (DBAUG), ETH Zürich. She is also a co-editor-in-chief of the DCE journal.\nKeywords: Physics-informed machine learning; Digital Twin Technology; Systems thinking; Data assimilation; Structural Health Monitoring\n\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X. Here are instructions for submitting an article to the journal."
  },
  {
    "objectID": "posts/11_engineering_emulators_DTs/engineering_emulators_DTs.html",
    "href": "posts/11_engineering_emulators_DTs/engineering_emulators_DTs.html",
    "title": "Engineering, Emulators, Digital Twins and Performance Engineering",
    "section": "",
    "text": "This blog offers informed opinions and perspectives relating to nascent technologies in data-centric engineering. Professor Ron Kenett discusses his recent DCE webinar, which covers how statistical tools can be used to build Digital Twins in engineering applications.\n\nNew developments in sensing technologies, big data, storage, and advanced analytics, all affect modern systems engineering. In the most recent DCE webinar, we discuss changes at a conceptual level to consider design optimisation and performance engineering — while aiming for responsive monitoring, diagnostic, prognostic, and prescriptive capabilities. We present a new modelling approach, stochastic emulators, and show how emulators can be used to complement finite element and computational fluid dynamic computational models. An organising framework for this conceptual shift is provided by digital twins, introduced through case studies.\n\n\n\nPhoto by Priyadharshan Saba on Unsplash\n\n\nThe first study is based on digital twins of train locomotives. The approach combines individual representations of brake systems, safety valves, and suspension frames. These digital twins enable monitoring, diagnostic, prognostic and prescriptive analytics of locomotives, to enhance safe and cost-effective condition-based maintenance. A second, more pedagogical example, is based on a combustion piston simulator that derives cycle time performance, after setting up seven input variables. The piston is introduced and integrated in notebooks offering code in R, JMP and Python. These notebooks use the piston simulator to cover topics such as statistical process control and statistical design of experiments.\nWe refer to the piston simulator to demonstrate the concept of a stochastic emulator — where a model is used, with variability in the inputs — to generate a model of the system variability. Typically, this representation is derived from hyper Latin cube computer experiments and Gaussian process models.\nAn essential characteristic of stochastic emulators is that they provide surrogate models of finite element methods (or dynamic computations) adequate for operational digital twins. Investing in digital assets requires a refocus of engineering efforts, from engineering of design to engineering of performance. We believe that this refocussing will characterise engineering science in future years. The webinar provides a glance at the potential performance capabilities.\n\nCompeting Interest: Professor Ron Kenett is Senior Research Fellow at the Samuel Neaman Institute, Technion, Haifa, Israel, Chairman of the KPA Group, Israel, Chairman of the Data Science Society at AEAI and Research Professor at the University of Turin, Italy.\nKeywords: Physics-informed machine learning; Digital Twin Technology; Systems thinking; Data assimilation; Structural Health Monitoring\n\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X. Here are instructions for submitting an article to the journal."
  },
  {
    "objectID": "posts/07_dynamic_fleets/dynamic_fleets.html",
    "href": "posts/07_dynamic_fleets/dynamic_fleets.html",
    "title": "Managing Fleets Dynamically Will Be Possible, Thanks to Digitalisation",
    "section": "",
    "text": "This blog offers informed opinions and perspectives relating to nascent technologies in data-centric engineering. Adolfo Crespo del Castillo discusses the digitalisation of fleet asset management, suggesting how managing the maintenance of train fleets dynamically will raise efficiency levels and reduce maintenance costs.\n\nAsset management and maintenance are entering an era of constant change following the technological advances that constitute digitalisation. The rail sector’s maintenance of infrastructure and moving assets presents both great challenges and business opportunities. The ecosystem is composed of stakeholders including OEMs (Original Equipment Manufacturers), network operators, and third-party companies that share responsibilities along the value chain. This is a discussion of how maintenance strategies for these ecosystems can benefit from digitalisation, with analytics and models that run in real-time, thanks to the data collected from critical systems of operational trains. According to the white paper from McKinsey Digital, The rail sector’s changing maintenance game, the global maintenance market is currently estimated at 45 to 50 billion per year, with condition-based maintenance and predictive technologies suggesting an efficiency gain of 15–25%. At the end of the day, this could reduce maintenance costs by up to EUR 4 billion for rail operators, up to EUR 2 billion for rolling stock OEMs, and up to EUR 4 billion for third parties.\n\n\n\nTalgo high-speed train fleet, UK\n\n\nUnderpinned by the IoT and smart sensors, new systems place data at the core of asset management processes, allowing the development of powerful decision-making models, supported by (near) real-time and in-situ measurements. In turn, digitalisation requires more sophisticated ways of managing assets, as streaming information must be integrated with engineering knowledge to support complex decision-making dynamically. The data themselves are only the ‘combustible for the motor’.\nFollowing this paradigm shift, companies are developing precise capabilities to detect and predict behaviours or failures in their valuable assets. On the other hand, as MIT Sloan Business school suggests, it is strategy and not technology that drives digital transformation. In most cases, when companies try to scale their algorithms to manage multiple assets and link them to other business dimensions, the level of maturity remains low. The rail sector is not an exception to this rule, particularly for locomotives, as maintenance involves numerous restrictions from multiple stakeholders.\n\nDeveloping a strategy to manage fleets dynamically\nThe main challenge when managing fleets is to recognise the opportunities offered by data. Beyond this, any information must be utilised while respecting the existing company structure and systematic activities — a pragmatic and applicable plan must be presented, otherwise, an unconstrained strategy might not consider restrictions: regulations or safety policies, for example. Such an approach is uninformed and naïve when it comes to generating a real impact in modern industrial applications.\nTogether with maintenance programmes, realistic boundaries need to be defined and classified. In my Doctoral Thesis at the Institute for Manufacturing of the University of Cambridge, a proof of concept was presented in collaboration with Talgo (Patentes Talgo S.L.U). An essential commitment for Talgo’s Smart Maintenance team was to develop business requirements in view of data availability, the existing condition-based maintenance programmes, and the development of an optimisation model that provides answers to the central planner. These answers should be provided dynamically for everyday operation — allocating trains to operation, maintenance, or staying idle — respecting the existing systematic stoppages opportunistically, to maximise usage and minimise cost. In simple terms, the objective is to scale from single-asset condition management and PHM (Prognostics and Health Management) to the maintenance of fleets dynamically.\n\n\nThe beginning of an exciting journey\nThis has been a discussion of the vision of a dynamic and responsive management strategy for valuable fleets, based on data, with humans in the decision-making loop. Firstly, while there are models that provide accurate and local predictions, they are not scaled to the fleet level. Second, there are multiple complexities associated with the fleet level, and there is a lack of solutions since we have not yet scaled our tools for data analysis. A gap must be overcome, likely by a hybrid solution, combining predictive and preventive maintenance approaches. The journey begins here, and there is ample opportunity to start walking.\n\nCompeting Interest: Adolfo Crespo del Castillo is a third-year PhD candidate at the University of Cambridge, developing the proof of concept for his thesis with Patentes Talgo S.L.U.\nKeywords: Dynamic Fleet Management; Dynamic Maintenance Management; Asset Management; Digitalisation; Rolling Stock; High-Speed Trains\n\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X. Here are instructions for submitting an article to the journal."
  },
  {
    "objectID": "posts/02_materials_informatics/materials_informatics.html",
    "href": "posts/02_materials_informatics/materials_informatics.html",
    "title": "Materials Informatics: Urgently Needed for a Sustainable Future",
    "section": "",
    "text": "This blog introduces a peer-reviewed article in the open-access DCE journal. Hannah Melia (Citrine Informatics) discusses the growing field of Materials Informatics, its potential for meeting environmental goals, and the role of open data in accelerating research.\n\nThe production of materials and chemicals is estimated to cause around a quarter of greenhouse emissions. Moving to biofeedstocks, enabling a circular economy, and increasing the efficiency of material production are all, therefore, important in meeting global carbon-neutral targets. Materials are part of the products we need to help us live more sustainably — contributing to decarbonization of the grid, electromobility, carbon capture, and so on. The development of transformative technologies to mitigate our global environmental and technological challenges will require significant innovation in the design, development, and manufacturing of advanced materials and chemicals.\n\nDeveloping new and qualified materials for use in critical applications remains a lengthy process; twenty years from discovery to market is common. To achieve innovations faster than traditional (human) intuition guided methods, we must transition to an informatics paradigm. Synergies between data science, materials science, and artificial intelligence can be leveraged to enable transformative, data-driven discoveries through the use of predictive models and digital twins.\n\nMaterials Informatics\nMaterials informatics (MI) is a growing field in the materials and chemicals industry. The field applies the principles of informatics to improve the understanding, use, selection, development, and discovery of materials. Common industrial applications include:\n\nThe discovery of new materials that have specific target properties\nOptimisation of the composition or processing parameters of existing materials\nIdentification of formulations that simultaneously meet performance, cost, and sustainability criteria\nIdentification of the most informative experiments to perform under budgetary requirements or other constraints\n\nYou can read case studies from companies such as Panasonic and Showa Denko here. However, the broad adoption of MI is hindered by barriers such as skill gaps, cultural resistance, and data sparsity.\n\n\nLooking forward\nMI software platforms are available and maturing, requiring less expertise in data science to use them; however, a basic understanding of machine learning and data engineering is necessary to make the most of the technology. As ever with software, rubbish in, rubbish out. This means that if researchers in materials and chemicals across industry and academia are to accelerate their investigations, both undergraduate and continuous professional development courses in MI need to be widely available.\nThe educational process needs to reach those who remain sceptical to the power of MI or those who worry that it will take over their jobs. It is important to emphasize that machine learning is not necessarily a black box and that the domain knowledge of experts is what enables machine learning to be used on small, sparse datasets such as those found in materials and chemicals.\nMaterials and chemicals datasets are expensive to acquire, with a single data point sometimes taking months and costing tens of thousands of pounds. The resultant information is very context-specific. A research team in the next lab may not be able to interpret and reuse data if the exact testing circumstances are not included in the metadata. In general, the more high-quality data available, the better machine learning models perform. It is important that future datasets are stored according to FAIR principles, something that can be encouraged by research funding bodies. Just as with the human genome project, access to high-quality data could have a transformative, catalysing effect on materials research.\n\n\nThe publication in Data-Centric Engineering\nThe perspective DCE paper is written with my colleagues Eric Muckley and James Saal. We discuss the importance of materials informatics for accelerating technological innovation, describe current barriers and examples of good practice, and suggest how researchers, funding agencies, and educational institutions can help accelerate the adoption of MI toolsets for science in the 21st century.\n\nCompeting Interest: Hannah Melia is a Product Management Consultant for Citrine Informatics, a company providing an artificial intelligence platform to accelerate materials and chemicals development.\nKeywords: Materials; DCE; Open Data; Artificial Intelligence; Sustainability\n\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X. Here are instructions for submitting an article to the journal."
  },
  {
    "objectID": "posts/04_safer_seas/safer_seas.html",
    "href": "posts/04_safer_seas/safer_seas.html",
    "title": "Working Together for Safer Seas",
    "section": "",
    "text": "This blog offers informed opinions and perspectives relating to nascent technologies in data-centric engineering. Adrian Clifton (HiLo) explains why data sharing in the maritime industry is critical to accurately predict and prevent high-impact incidents, saving time, money and lives.\n\nThe world is more connected than ever before, thanks to the Internet and intelligent technology powered by it. From 2010–2016 there were on average 640,000 people online for the first time every day: an extra 27,000 every hour. Things have only accelerated since then, with the growth of mobile 4G and 5G data: between April 2020 and April 2021, more than 330 million more people gained online access.\n\n\n\nPhoto by Venti Views on Unsplash\n\n\n\nThe rise of information sharing worldwide\nNot only is access to information growing, but the amount, quality and depth of that data is also increasing. The Internet offers solutions to almost every issue. Almost every possible query has already been asked, and the answer is available at a click of a button.\nInformation is also available through many different methods, such as:\n\nReading blogs\nSearching through social channels\nWatching video-sharing platforms\nBrowsing web pages for specific areas of interest\n\nThe growth of connected devices — ‘The Internet of Things’ — means that people don’t even have to search for the answers. This is because machine learning, a form of artificial intelligence (AI) and computer science, can now be used by machines to imitate the way that humans learn through experience.\nThis allows them to:\n\nImprove their future performance without needing new data to be entered manually\nSolve issues and surpass obstacles by connecting systems that had no way of communicating before\n\nIndustries such as aerospace, rail and nuclear energy are prime examples of how this technology benefits organisations. With airlines, for example, this connectivity can:\n\nHelp avoid delays\nDetect technical issues through machine learning, and predict- where repairs will be needed in the future\nQuickly source new components when they are needed\nMake flight routes faster and safer\n\nThe knowledge all this data generates creates new opportunities for improvement across every industry. How can mariners and maritime organisations use this immensely powerful data to improve our operations?\n\n\nAn increasingly connected maritime industry\nThe marine sector is greatly increasing its use of automation and data sharing. This decreases the need for marine personnel to perform tasks manually, and in turn, reduces human error. This is a crucial safety improvement for the industry. Gathering data through technology is faster, simpler, more accurate and provides future-proofing (as connected equipment learns best practices and improves delivery).\nPotential marine safety risks (especially rare ones) have traditionally been very challenging to anticipate. However, if a mariner needing to solve a problem with a piece of shipping equipment has access to connected technology, they can access real-life data and experience to see if anyone else has experienced the same issue; and more importantly how to solve it. Even if they cannot discover the exact solution, they have additional data to help them troubleshoot the problem.\n\n\nSharing is caring for all\nCollecting and using this information within your organisation is one thing. However, far greater benefits will be achieved by improving safety and efficiency across the whole industry. In order to achieve this, data must be shared and collated. This greater volume of information provides much more accurate predictions.\nHappily, the maritime industry is starting to work more closely together by building relationships, communication and connectivity, in order to make the most of the hugely valuable information resources available. For example, technology can utilise real-life experience from seafarers themselves. Information can be gained, input and shared on how previous fixes have been identified and applied, and how these can be used in the future. By sharing this data, human error can be reduced and solutions can be fed back to prevent incidents in the future.\n\n\nThe big environmental question\nMany different groups within the maritime industry are driving this connectivity. The aim is to achieve industry-wide improvements in key areas including:\n\nImproving and managing safety\nIncreasing efficiency\nPerhaps the most important and challenging item on the agenda: environmental impact\n\nA connected industry brings benefits in many areas:\n\nImproved safety and efficiency of maritime operations for all, due to fewer incidents on the seas\nEquipment driven by connected technology is less likely to develop faults, meaning industry professionals are freed up to drive businesses forward\nSaving time and money in these areas allows you to focus on new challenges\n\nEnvironmental improvements are obviously a very high priority and will only become more so. Industry conversations on how to respond are critical.\nData connectivity can play a huge part by demonstrating the impact of new technologies and techniques such as:\n\nMore efficient fuel types\nFuel management optimisation techniques\nUpgrades of navigation systems to keep ships at sea longer\nDual-fuel engines that can use LNG (liquefied natural gas) or marine diesel fuel\nFuture environmentally-friendly fuels when they emerge\n\n\n\nA connected maritime future is a better one\nThe more knowledge both mariners and their assistive technology have of potential scenarios and how to solve them, the lower the risk of errors when managing them. By continuing to build connectivity and share data within the industry, professionals and organisations can learn from each other to improve life for everyone on the seas — from efficiency to safety to environmental improvements.\nThe more connected the industry becomes, the safer seafarers will be. The more that high volumes of data from different vessels, scenarios and organisations are shared and analysed in a single place, the more accurately high-impact incidents can be predicted.\n\n\nBuilding a decision-making system for maritime safety\nHowever, this is not at all easy to achieve. Shipping companies have traditionally been very reluctant to share safety data, for both commercial sensitivity and reputational reasons. This means publicly available data for the industry is very limited. As noted above, to make a big data approach effective you need not only a wide variety of data, but a significant amount of it. This is because:\n\nYou need a sufficient volume of data for patterns and trends to emerge and be verifiable\nThe greater amount of data you have, the better chance you have of finding the unusual, unexpected information that can be a critical game-changer for your approach\n\nThe answer lies in pulling this data together into a single independent source, and maritime technology company HiLo has done just this. They have created the industry’s first big-data-driven decision-making system, combining info from over 4,000 vessels with world-leading statistical analysis.\nThe data have been anonymised, therefore ensuring company reputations are protected and providing them with greater encouragement to share it. This has broken the largest barrier to data sharing, demonstrated by the fact that those 4,000 vessels are drawn from more than 55 competing shipping companies, who would never have been comfortable sharing such information previously.\nAdvantages include:\n\nThe dataset is large in both volume and scope, coming as it does from sources including incident management systems, audits and equipment\nIt can be used to generate expert fleet risk analysis, presented clearly through simple, customised dashboards that highlight the biggest risks\nTherefore maritime companies can prioritise budget and resources effectively to make the most important safety changes to protect crews and vessels across the seas\n\n\n\nSafety success on the seas\nThe success of this decision-making system has been outstanding. 2021 figures saw a massive yearly improvement for the companies whose vessels are part of the HiLo fleet. Highlights included:\n\n£3 million cost reduction from issues prevented or mitigated early\n12% reduction in actual/potential high-impact incidents across the fleet\n8% reduction in serious injuries to crew members across the fleet\n5% reduction in major spills across the fleet\n308 fewer actual/potential high-impact incidents for tankers\n135 fewer actual/potential high-impact incidents for bulk carriers\n\n\n\nBetter together\nIt can be seen, then, that maritime companies simply cannot make quantum leaps in safety on their own. The amount and range of data from even a large individual organisation in isolation will not compare to what can be achieved together. To protect the industry through its ever-changing challenges, data sharing is critical.\n\nCompeting Interest: Adrian Clifton writes for HiLo, a maritime safety decision-support company which works to save time, money, and lives at sea through data analysis.\nKeywords: Maritime Safety; Marine Risk Management; Decision Support; Technology\n\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X. Here are instructions for submitting an article to the journal."
  },
  {
    "objectID": "posts/01_guidelines/guidelines.html",
    "href": "posts/01_guidelines/guidelines.html",
    "title": "The DCE Blog: Instructions for Authors",
    "section": "",
    "text": "[Updated: Oct-2024]\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X.\nThe DCE blog is a channel for sharing informed opinions and perspectives relating to research and technology in data-centric engineering. Articles should explore the benefits and implications of data science when improving the reliability, safety, and efficiency engineered systems.\nSuggested blogs:"
  },
  {
    "objectID": "posts/01_guidelines/guidelines.html#author-profile",
    "href": "posts/01_guidelines/guidelines.html#author-profile",
    "title": "The DCE Blog: Instructions for Authors",
    "section": "Author profile",
    "text": "Author profile\nProspective authors can include practitioners or researchers working at the interface of data science and engineering. Authors can include academics in universities and similar institutions, or professionals engaged with transformative data practices in industry. We actively encourage individuals with diverse backgrounds to propose posts. We apply the same authorship principles as the DCE journal. The editor cannot mediate disputes, any changes in authorship will require the approval of all current and proposed authors. Authors should declare any relevant competing interests in their blog. Posts associated with sponsorship will be acknowledged as such. We expect all authors to follow the guidelines and recommendations below. To submit a blog for our consideration, please contact us at dce@cambridge.org"
  },
  {
    "objectID": "posts/01_guidelines/guidelines.html#format",
    "href": "posts/01_guidelines/guidelines.html#format",
    "title": "The DCE Blog: Instructions for Authors",
    "section": "Format",
    "text": "Format\nWe seek to publish pieces between 500 words and 1500 words. If longer, we may consider publication in series, at the editor’s discretion. If a blog is based on a published article, the authors should provide a link to the DCE publication."
  },
  {
    "objectID": "posts/01_guidelines/guidelines.html#style",
    "href": "posts/01_guidelines/guidelines.html#style",
    "title": "The DCE Blog: Instructions for Authors",
    "section": "Style",
    "text": "Style\nThe main goal is brevity and focus. Outside of academic and tutorial posts, language should be understood by the general public with interests in new technologies of engineering data-science. Technical terms and jargon should be defined when used, ideally avoided if the terminology does not explicitly contribute to the discussion. The author should deal directly with the matter at hand, avoiding long introductions. The text should stand in itself: whilst you can include references, the post should not heavily rely on external sources to be understood."
  },
  {
    "objectID": "posts/01_guidelines/guidelines.html#evidence-and-referencing",
    "href": "posts/01_guidelines/guidelines.html#evidence-and-referencing",
    "title": "The DCE Blog: Instructions for Authors",
    "section": "Evidence and referencing",
    "text": "Evidence and referencing\nThere is no need for academic citation styles. Links are sufficient, which should offer access to published works that provide details or examples. It is always better to direct readers to open-access sources. The author can provide links in-text to the editorial team, who will add them to the final publication. Although the citation format is informal, authors should take care to appropriately acknowledge any sources and avoid plagiarism."
  },
  {
    "objectID": "posts/01_guidelines/guidelines.html#visual-resources",
    "href": "posts/01_guidelines/guidelines.html#visual-resources",
    "title": "The DCE Blog: Instructions for Authors",
    "section": "Visual resources",
    "text": "Visual resources\nVisual resources (including charts, diagrams and graphs) can be useful to introduce and summarise content. They should be labelled appropriately and connected to the sources of primary or secondary data. The editorial team may add images to blog posts to facilitate reading. Preferably, the author has proper rights of use and access to specific images and seeks to have them included in the publication.\nFor tutorial and/or academic posts, one can display code,\ndef hello_world():\n    print(\"hello, world!\")\nequations,\n\\[\n\\ddot{x}+\\delta \\dot{x}+\\alpha x+\\beta x^3 = f(t)\n\\]\nas well as graphs, etc. All the functionality of qaurto is available, see the documentation for details."
  },
  {
    "objectID": "posts/01_guidelines/guidelines.html#editorial-input",
    "href": "posts/01_guidelines/guidelines.html#editorial-input",
    "title": "The DCE Blog: Instructions for Authors",
    "section": "Editorial input",
    "text": "Editorial input\nAll posts will be assessed by the DCE editorial team before a decision is made about publication. Our communications editor may suggest changes to authors, including language, sources and structure. If the changes needed are major, we may suggest a different deadline or theme for their consideration. Corrections and updates to the blog are at the discretion of the DCE editorial team. Authors will be informed/consulted on any changes. All changes, including removal, will be made transparently."
  },
  {
    "objectID": "posts/01_guidelines/guidelines.html#post-publication",
    "href": "posts/01_guidelines/guidelines.html#post-publication",
    "title": "The DCE Blog: Instructions for Authors",
    "section": "Post-publication",
    "text": "Post-publication\nThe DCE team will encourage the author to promote the published article across their personal social media profiles and any associated institutional accounts - including universities, research groups and others. DCE may write additional material and create graphics or videos to promote the contribution. The blog post might be used across all DCE communication channels: the website, newsletter, and social media.\n\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X. Here are instructions for submitting an article to the journal."
  },
  {
    "objectID": "posts/03_sensing_methane/sensing_methane.html",
    "href": "posts/03_sensing_methane/sensing_methane.html",
    "title": "Remote Sensing for Methane: Monitoring and Reducing Greenhouse Gas Emission",
    "section": "",
    "text": "This blog offers informed opinions and perspectives relating to nascent technologies in data-centric engineering. Matthew Jones (Shell) and Philip Jonathan (Shell, Lancaster University) discuss the importance of monitoring methane to reduce global warming, different sensing technologies, and source location methods for unusual methane signals.\n\nMethane (CH₄) is a key greenhouse gas and absorbs thermal radiation more effectively than carbon dioxide (CO₂). Hence, even though it is much less abundant than CO₂ in the atmosphere, CH₄ still traps about half the overall heat trapped by CO₂. The relatively short lifetime of atmospheric CH₄ also means that any reductions in anthropogenic CH₄ emissions will translate into a more immediate reduction in warming. Measuring atmospheric CH₄ to identify and characterise sources of man-made emissions is therefore of obvious interest. This blog summarises the sensing technologies and inference methods used recently for methane monitoring by us and our colleagues; you can click on the hyperlinks for more information.\n\n\n\nA CH₄ plume in Algeria (from TROPOMI)\n\n\n\nSensing technologies\nThere are many CH₄ sensing technologies appropriate for different types of application, at different spatial and temporal scales. On a global scale, spectrometers such as TROPOMI (on ESA’s Sentinel-5P satellite) provide column integrated concentration measurements for CH₄ at a ground pixel resolution of around 7×7 km² on a daily basis. For continuous monitoring of emissions from a specific location (like a gas processing facility) of the order of 1 km², optical line-of-sight sensing is a possibility. Various types of point sensors are also available; some of these are cheap to deploy, but this tends to come at the expense of accuracy. Manned aircraft and drone-based optical sensing provide flexible alternatives when continuous monitoring is difficult or not desired.\n\n\nSource location\nIn principle, we can use satellite-based measurements directly to estimate the spatial distribution of CH₄. Line-of-sight sensing is useful for anomaly detection, to ensure that no unusual CH₄ signals occur. It is sometimes useful to infer the presence of CH₄ by measuring other gases whose concentrations are coupled to those of CH₄. If we have enough knowledge about the wind field, we can use remote measurements to triangulate the location of a source, and estimate its characteristics; this process is commonly called inversion. Wind field data is used within a dispersion model to describe how CH₄ propagates from the source to the measurement location. Such a model can be very simple (like a Gaussian plume) or more complex (such as a numerical solution to the differential equation describing gas advection and diffusion). Source characteristics and associated uncertainties can be calculated using Bayesian inference; because this can be computationally challenging, we require efficient numerical methods or approximations to achieve it. It is usually necessary to estimate the variation of background CH₄ in space and time as part of the inversion.\nSometimes, especially for satellite monitoring, we can even identify the spatial profile of a CH₄ plume emerging from the source, as in the figure; estimating the source location, in this case, is pretty simple! Sometimes, aircraft gather measurements on helical trajectories around a known source location, at increasing altitudes, and then use a mass balance to estimate the amount of CH₄ emerging from the ground area corresponding to the helix. With the development and deployment of improved CH₄ monitoring technology, from more and higher-resolution satellite-based sensors to cheaper, more accurate point sensors, our ability to quantify atmospheric methane is improving all the time.\n\nCompeting Interest: Matthew Jones is a Statistician at Shell; Philip Jonathan is Chief Statistician at Shell, and Chair of Environmental Statistics and Data Science at Lancaster University.\nKeywords: Monitoring; Remote Sensing; DCE; Sustainability\n\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X. Here are instructions for submitting an article to the journal."
  },
  {
    "objectID": "posts/14_ECross/EJ_MJ.html",
    "href": "posts/14_ECross/EJ_MJ.html",
    "title": "A Spectrum of Physics-informed Gaussian Processes for Engineering",
    "section": "",
    "text": "This blog introduces a peer-reviewed position paper in the open-access DCE journal. Prof. Elizabeth Cross and Dr. Matthew Jones from the University of Sheffield discuss Gaussian process regression for physics-informed machine learning.\nOver the course of the last several years, the increasing interest that we are seeing in data-centric engineering has largely stemmed from a growing capacity to acquire data from the physical world around us. Advances in sensing and storage hardware now make it possible for us to collect vast amounts of data in fairly short time periods, allowing us to make use of artificial intelligence (AI), and particularly machine learning (ML), to analyse and predict behaviours of engineering systems.\nDespite these advances, there are many cases where AI isn’t ready to be a good engineer right now. Whilst it is true that machine learning has seen us make some remarkable gains, we have very quickly arrived at a new set of problems that we as engineers need to overcome to be able to use the technology well.\nIn this article we focus on two significant and interlinked barriers to using ML to its full potential in an engineering context - a lack of data about all behaviours of interest from our critical structures and systems (even if what we do have is “big”), and the inability to use the data we have to make predictions about unmeasured conditions (extrapolation). Add to these challenges the fact that data from the real world are often noisy, corrupted, not direct observations of the behaviours we actually care about, and often contain missing values, you soon start to get the idea that there are many pieces of the puzzle still missing when attempting to characterise our physical world through monitoring data. We think that part of the solution to these problems is to try and build what we know already - some knowledge of physics - into machine learning algorithms, and later we will show how this can help through some examples from structural dynamics."
  },
  {
    "objectID": "posts/14_ECross/EJ_MJ.html#footnotes",
    "href": "posts/14_ECross/EJ_MJ.html#footnotes",
    "title": "A Spectrum of Physics-informed Gaussian Processes for Engineering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDid you make this up and do we really need another term for hybrid modelling?’ yes we did and no we don’t, this is a blog, don’t take it too seriously please, imagine a nice cup of data tea with some physics stirred in.↩︎"
  },
  {
    "objectID": "posts/12_built_environment/built_env.html",
    "href": "posts/12_built_environment/built_env.html",
    "title": "Why the Built Environment Needs Innovations in Data-Centric Engineering",
    "section": "",
    "text": "This blog offers informed opinions and perspectives relating to nascent technologies in data-centric engineering. Dr Ramaseshan Kannan (Head of Computational Science at Arup) discusses how, despite first appearances, the built environment requires rapid developments in Data-Centric Engineering.\n\nIt is no secret that the built environment faces enormous challenges globally and from a planetary perspective. On the one hand, the sector is responsible for up to 40% of all carbon emissions; on the other, there is increased demand on our built infrastructure — from rapid urbanisation and natural calamities such as earthquakes, floods, or hurricanes. In turn, there is a two-pronged challenge of decarbonisation and resilience. It is here that Data-Centric Engineering — which I see as an interdisciplinary field including computational science, data science, computational statistics, optimisation, engineering and more — will have a crucial role to play.\n\n\n\nCredit: Arup\n\n\nThe built environment is often seen as a field that doesn’t need to be on the cutting edge of science. But this misperception couldn’t be farther from reality! There is a variety of complex problems that need a range of computational techniques to be solved. A number of these are unsolved problems or might be on the research frontier at low Technology Readiness Levels. As such the techniques are effectively non-existent or unavailable to the industry. On their part, research funding bodies and industry must realise these are not Instagram-style problems — rather they need ‘deep tech’ innovation and investment.\nSo, what kinds of AI and science does the built environment need to tackle emerging challenges? Through the lens of my work at Arup and our collaborators from academia and industry, here are a few examples.\nIn each case, I present a set of broad problems in the application domain, and hypothesise an (incomplete!) set of innovations we might need to leverage.\nWe have ageing assets whose design, condition or degradation are undetermined. Our products are prototypes that are prone to highly adverse consequences of ‘getting it wrong’.\n\nScientific problems: new ways to blend physics and data; new techniques to predict and prognose; alternative ways to reconstruct system behaviour from noisy, disparate data sources; techniques to simulate and understand physical behaviour — especially in safety-critical scenarios.\nInnovations: new forms of uncertainty quantification; scientific machine learning; statistics; computational mechanics; novel material models; multi-modal learning.\n\nWe want to prototype designs faster to aid a better exploration of design space — especially in the early stages of a project ­ — to reduce embodied carbon by using materials more efficiently in Pareto optimal settings.\n\nScientific problems: new ways to rapidly simulate and optimise for multiple disciplines, while drawing high fidelity machine learning-enabled simulations.\nInnovations: new ways to reduce computational requirements through surrogates or reduced order models; probabilistic machine learning to quantify uncertainties; techniques to solve combinatorial optimisation; data-efficient learning in high dimensional spaces.\n\nWe have sparse data, and our designs arise from a creative process that owners don’t want to share. Each building, bridge, or city masterplan is unique.\n\nScientific problems: learning from spatial and geometric data such as 3D models; leveraging trained models across projects, and doing so while respecting intellectual property.\nInnovations: new forms of transfer learning, learning on graphs, differential privacy, generative modelling.\n\nOur assets are often systems of systems, ingesting and producing vast amounts of disparate data. We must use this information to make better decisions.\n\nScientific problems: ways to mix many kinds of data — sensors, images, video, meteorological, anthropic, geospatial, geotechnical, environmental — to support decision-making for growing urbanisation and infrastructure, and to become resilient to climate events.\nInnovations: multi-modal learning, multi-fidelity, multi-hierarchical modelling, belief networks and much more!\n\nThe infrastructure we design and maintain has ever-changing demands. Whether it be a century-old bridge, carrying far heavier traffic compared to the original specification, or an operator looking to provision offshore wind farms. We must deal with unpredictability in how assets are utilised.\n\nScientific problems: better ways to predict and account for uncertainty in demand.\nInnovations: better techniques for demand forecasting, uncertainty quantification, agent-based modelling, stochastic optimisation, and sequential decision problems.\n\n\n\n\nPhoto by Erik van Dijk on Unsplash\n\n\nI’ve only covered a tiny section of the entire set of possible areas!\nWe need a fundamental shift in the technology stack for dealing with data and computation at scale. As an example, a typical parametric study to solve an inverse problem (or to train a surrogate) might involve several thousands of runs of a simulation solver, and a large dataset to accompany. Traditional supercomputing and HPC facilities (whilst popular within academia) are usually uncommon in the built environment industry. The flexibility and agility afforded by public cloud services appear the most optimal solution to this problem. The advantages of high-performance computing on the cloud are hard to ignore; however, doing so needs modernisation, upskilling, and capability building. As an example, Arup works closely with AWS to solve some of the above problems at scale.\nSo, with a rich tapestry of applied and fundamental problems to be solved, work is needed at both an application level as well as methodological level.\n\nCompeting Interest: Dr Ramaseshan Kannan is Head of Computational Science at Arup and a Royal Academy of Engineering Industrial Research Fellow.\nKeywords: Computational Science; Built Environment; Scientific Machine Learning; Sustainability\n\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X. Here are instructions for submitting an article to the journal."
  },
  {
    "objectID": "posts/05_auto_supply_chains/auto_supply_chains.html",
    "href": "posts/05_auto_supply_chains/auto_supply_chains.html",
    "title": "Automating Supply Chains: How Heavy is My Shopping?",
    "section": "",
    "text": "This blog offers informed opinions and perspectives relating to nascent technologies in data-centric engineering. Adrià Salvador and Ponç Palau (Glovo) present the development of Glovo’s language modelling tools, which inform their logistics algorithms by estimating the weight of deliveries.\n\nGlovo is one of Europe’s largest on-demand delivery platforms, operating in more than 25 countries, and 1500+ cities. Due to increased demand in the grocery sector, in recent years the company has shifted from being a mostly food delivery service to a service where groceries play a major role.\nOne of the biggest challenges that we faced at Glovo when moving to the grocery delivery space was the increased prevalence of big orders — orders that would not fit in the backpack of our couriers, or orders that were too heavy to be transported. This made sense: when ordering groceries online, customers order more items than, for instance, when they are getting their office lunch.\n\nAppropriately detecting and handling big orders was crucial. A big order that goes undetected has significantly worse unit economic costs than a normal order: agents have to get involved, delivery times increase, and user experience deteriorates. Worse than that, if a small order is incorrectly flagged as a big order, it costs us nearly double to process, as we have to dispatch two or more couriers to handle it.\nOur initial solution was to ask big grocery stores to label their products by providing us with weight and volume measurements. This was successful in labelling a fraction of our product offering and allowed for a certain level of automation. However, it proved hard to scale: we operate with thousands of Small and Medium-sized Enterprise partners in a massively diverse environment (from eastern Europe to sub-Saharan Africa). This meant that asking our partners to manually label all their products was not a realistic strategy. We needed a way to fully automate our weight and volume estimations.\n\nThe importance of Data-Centric AI for our production model\nWhen looking at the labelled data provided by our partners, we realised that they contain a significant amount of wrongly inputted data. Typos, measurement unit misunderstandings, and other mistakes were prevalent.\nThe first step that we took to fix this was to clean the large dataset of labelled data until we obtained a fully reliable dataset. Very much in the spirit of data-centric AI, we have consistently observed that the biggest gains in accuracy result from curating the datasets used to train our models.\nTo do this, we used a combination of isolation forests and distribution-based outlier detection to prune out samples with clearly incorrect labels. We first removed the values that were extremely high in our dataset using high-level business-related information. However, these outliers were not the single source of error in our dataset. We still had samples that were incorrectly labelled due to confusion with measurement units: grams instead of kilograms, meters instead of millimeters, etc. In order to detect and correct these samples, we applied the following rules:\n\nDuring our EDA, we noticed that stores often had consistent labelling errors (for example, confusing grams and kilograms or meters and centimetres). Therefore, we grouped our samples by store and then analysed the median weight and volume associated with that store. For the case of weight, in the lower and upper quantiles, we could detect stores that mislabeled their product due to an error with measurement units. For the volume case, it was more complex because the final label (volume) was the result of the product of height, width, and length: an error in a single dimension could corrupt the final label. To solve this problem, we ran an isolation forest algorithm to detect outliers on the data (grouped by store) in the 3D space of height, width and length. These allowed us to spot errors in each dimension and correct them.\nWe used regex methods and the quantulum library to extract any present amounts of weight or volume in the product name. For example, for the product name “Coca Cola 330 ml’’ this processing would extract 0.33 l. We used this output as a prior to update the distribution of possible weights and volumes with a heuristic.\nFinally, we manually reviewed the examples that had the highest variance and highest entropy of the labels, as well as the products that are bought most frequently, and labelled them correctly.\n\n\n\nThe architecture: Language Models come to play\nDue to engineering constraints, we decided to build a model that would take input features as solely the name of our products in natural language. The model would then have to predict the weight and volume of the given product with high precision.\nThe state of the art for NLP processing are Large Language Models, concretely, models based on the transformer (encoder-decoder) architecture. Initially, we tried to solve the problem using a vanilla architecture: a multilingual transformer with downstream fully connected layers. This showed strong performance, but we were convinced that we could improve the model using domain knowledge. We wanted to use the fact that we knew exactly what parts of the text contained strong information with regard to product dimensions.\nThis line of thought followed another important principle of production ML systems: when you have domain knowledge regarding the problem, you should use it. To do so, we used quantulum to extract normalised weight and volume measurements from product names. For example, for “Coca-cola 300 ml”, we extract 330 millilitres as a volume feature and normalize it to a common unit.\nThese extracted measurements enter the model through a separate, fully connected, tower (neural network). The output of the two towers (transformer and quantulum) is then merged and sent downstream through further fully connected layers. The final prediction is a measurement, either weight or volume, that then is aggregated and compared with our weight and volume limits, to determine whether the order is big.\n\n\n\nThe deep learning architecture that we ended up using in production. Fully connected layers represent the simplest component in deep learning architectures, corresponding to non-linear perceptrons.\n\n\nThis architecture has been extremely successful and provided a significant delta in accuracy, compared to using a transformer, to directly learn the weights and volumes in the dataset.\n\n\nDeploying LLMs in production at low latency\nWe deploy the model using our machine learning platform. In short: we dockerise the model and serve it through an API. We need the model to do predictions in low latency (response time) because it will be consumed by humans, and user experience gets significantly deteriorated with higher latency times.\nTo achieve a latency of approximately 100 milliseconds, we deploy the volume and weight models separately and use distilbert as a pre-trained model for our transformer tower. Distilbert is a conversion of the popular language model “BERT” that provides 60% better latency while preserving 97% accuracy. Autoscaling is handled by kubernetes.\n\n\n\nKnowing how much an order weighs is crucial to determine which transport type will be used\n\n\n\n\nEvaluation of the algorithm\nWe focus on measuring two types of metrics (i) offline model accuracy metrics such as mean absolute error and mean squared error, and (ii) online business-related metrics such as agent costs, courier complaints, etc.\nWe have run a backtest based on business metrics using our historical data for orders. Essentially, we look at which orders that were flagged as big by agents or couriers would be detected by our new algorithm. To do this, we try two different heuristics (i) using the language model algorithm only when weights and volumes data are missing, (ii) using the language model all the time, even when labels are available. The second heuristic performs more than 11 points (11% in absolute terms) in precision better than the first one.\nFor the second heuristic, we use the machine learning model as an anomaly detector. When weight or volume information is available, our backend compares it with the output of the model, and only if the model prediction is close to the weight stored in the backend, it considers the label valid. Otherwise, the network output is used. What this essentially means is that our network is more precise than our labels — as it has been trained with a well-curated dataset.\nWe need online business metrics because our big order labels are not perfect. Using the algorithm to analyze all our past orders, we observed that couriers had, on some occasions, taken orders that were heavier than what they were supposed to be. This means that if a courier takes an order without reaching courier support, we can’t really consider that order to be small. Similarly, we have observed instances of orders labelled as big by customer support that were in fact small orders. Both examples are prevalent and mean that we need to use a real-world A/B test to measure our algorithm’s impact.\nTo do so, we have scheduled to run an A/B test that will focus on a medley of indicative metrics that we know are directly related to the accuracy of our big order algorithm. These are related to agent cost and handling time, user experience, delivery time, courier complaints, and a metric that is crucial for our bottom line: cost per order.\n\n\nConclusion and future steps\nIn conclusion, at Glovo we have seen that:\n\nTransformers can be used for much more than traditional Natural Language Processing.\nA mid-size dataset of very well-labelled examples can help algorithms scale globally at high precision. Data-Centric techniques were the ones that provided the biggest jump in accuracy for our model.\nFor real-life situations, where data can be faulty, one can use deployed deep learning models as outlier detectors. Similar to what is done with autoencoders, a deployed model is a reflection of the distribution that is trained upon, and it can be used to find out-of-distribution examples.\nIf possible, one should always encode domain knowledge within the model architecture. After Data-Centric techniques, this was the second most important modification in terms of accuracy.\n\n\nCompeting Interest: Adrià Salvador is the Lead Data Scientist at Glovo’s CORE DS team. Ponç Palau is a Data Scientist in the CORE team.\nKeywords: Supply Chain Management; Language Models; Logistics; Natural Language Processing\n\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X. Here are instructions for submitting an article to the journal."
  },
  {
    "objectID": "posts/08_sharing_benchmarks/sharing_benchmarks.html",
    "href": "posts/08_sharing_benchmarks/sharing_benchmarks.html",
    "title": "Sharing Benchmarks to Improve Maritime Safety",
    "section": "",
    "text": "This blog offers informed opinions and perspectives relating to nascent technologies in data-centric engineering. Adrian Clifton (HiLo) explains why data sharing in the maritime industry is critical to accurately predict and prevent high-impact incidents, saving time, money and lives.\n\nBusiness benchmarking involves measuring your organisation’s safety against those of your competitors. It provides critical insights to analyse your current strengths and weaknesses — how to improve them — as well as opportunities and threats within your particular industry, and how you can respond to them.\n\n\n\nPhoto by Christopher Burns on Unsplash\n\n\nIn fact, benchmarking is a natural, often subconscious process; we perform it every time we make a choice, in order to compare elements that matter most to us, and to allow us to choose the best alternative. While not always accurate, it gives a starting point to assist our final decision. As a simple example, every time we buy something, we compare it against a mental benchmark of different elements to see if it is fit for purpose — a form of pattern recognition. This is often the most important consideration in our final decision to purchase, whether for domestic, leisure or business.\n\nWhy is business benchmarking important?\nIn order to ensure businesses are maintaining a competitive position within their industry, benchmarking is vital. It allows them to:\n\nSee where they stand in comparison to their competitors.\nDrill down into performance gaps, to see where they are leading and identify areas for improvement.\nMonitor company performance and manage change.\n\nAs members of a particularly fast-paced and ever-changing industry, with increasing environmental and safety pressures, it is vital that shipping companies continue to improve their benchmarking. When broken down into individual areas, benchmarking can help improve practices, increase efficiencies and above all drive for safer seas.\n\n\nWhat can go wrong?\nThere are hundreds of ways to benchmark — some more useful than others. For the most effective benchmarking, however, the data must be complete and accurate. The benchmarking data that has been available previously in the maritime industry is top-level — based on publicly available information. However, it should be no surprise that companies have often been cautious when releasing sensitive internal data, and this is a major problem for shipping companies when trying to build an effective benchmark — the full data is simply not available. As such, 3 major issues need to be resolved:\n\nComparisons made with competitors may be inaccurate, as they do not reflect the full picture in terms of data.\nCompanies may fail to draw the most valuable insights.\nWorst of all, they may be misled and take the wrong actions. These could be, at best, ineffective; or at worst, counterproductive, meaning reduced safety and efficiency.\n\n\n\nHow can you solve these issues?\nIn response to this, maritime technology company HiLo has produced a safety decision-making system, which shares the insights enabled by collected customer data. Our key focus is to make everyone safer at sea, without penalising our customers, so all of our data is carefully anonymised. In turn, customers can comfortably provide their internal safety data without fear of commercial repercussions. Companies can discover where they truly stand in relation to the industry at large through big data and expert, peer-reviewed analysis. By utilising this effective benchmarking tool, and the accompanying statistical analysis, companies can:\n\nSee the risks they face.\nConfirm risk-severity and the priority order for management.\nAnticipate issues, meaning they can take proactive action before such problems cause a serious safety incident.\n\nAll while sharing insights between stakeholders.\n\nCompeting Interest: Adrian Clifton writes for HiLo, a maritime safety decision-support company.\nKeywords: Maritime Safety; Marine Risk Management; Decision Support; Technology\n\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X. Here are instructions for submitting an article to the journal."
  },
  {
    "objectID": "posts/13_net_zero_aviation/aviation_AI.html",
    "href": "posts/13_net_zero_aviation/aviation_AI.html",
    "title": "Towards Net Zero Aviation with AI",
    "section": "",
    "text": "This blog offers informed opinions and perspectives relating to nascent technologies in data-centric engineering. Dr Pranay Seshadri (Georgia Institute of Technology) explores opportunities presented by the use of AI within the aviation industry, and introduces the The Net-Zero Aviation with AI mini-workshop.\nNext week, Georgia Tech will host the Net-Zero Aviation with AI mini-workshop, a groundbreaking collaboration between industry leaders, academics, trade associations, and government officials. This workshop, funded by the Daniel Guggenheim School of Aerospace Engineering and the United Kingdom Science & Innovation Network, reinforces the strong bilateral partnership between the US and the UK. It’s a testament to both nations’ shared commitment to achieving net-zero emissions by 2050.\nThe aviation sector, uniquely interconnected and global in nature, underscores the importance of such collaboration. After all, policies and actions taken in one corner of the world can ripple across the entire industry."
  },
  {
    "objectID": "posts/13_net_zero_aviation/aviation_AI.html#ai-a-catalyst-for-aviation-sustainability",
    "href": "posts/13_net_zero_aviation/aviation_AI.html#ai-a-catalyst-for-aviation-sustainability",
    "title": "Towards Net Zero Aviation with AI",
    "section": "AI: A Catalyst for Aviation Sustainability",
    "text": "AI: A Catalyst for Aviation Sustainability\nThe rapid advancements in artificial intelligence (AI) offer an unprecedented opportunity to make aviation more sustainable in the near future. We’re already seeing exciting examples of this in action:\n\nGoogle & American Airlines: Google’s AI-powered contrail prediction model helps pilots adjust flight paths, minimizing these heat-trapping clouds.\nGoogle & Lufthansa Group: Google’s collaboration with Lufthansa Group has resulted in an AI-powered flight planning tool that optimizes routes for maximum fuel efficiency.\nAssaia: This innovative startup uses AI-powered cameras and sensors to monitor airport ground operations, detecting excessive emissions from auxiliary power units (APUs) and helping airlines cut down on unnecessary fuel burn.\n\nThese are just a few of the ways AI is being leveraged to make aviation greener. The possibilities are vast, ranging from more accurate carbon emissions calculators to optimizing sustainable aviation fuel (SAF) production and even tracking waste from aircraft and airports."
  },
  {
    "objectID": "posts/13_net_zero_aviation/aviation_AI.html#small-steps-big-impact",
    "href": "posts/13_net_zero_aviation/aviation_AI.html#small-steps-big-impact",
    "title": "Towards Net Zero Aviation with AI",
    "section": "Small Steps, Big Impact",
    "text": "Small Steps, Big Impact\nEven small steps in the right direction, when amplified across fleets and airports worldwide, can lead to significant emissions reductions. The Net-Zero Aviation with AI mini-workshop is a crucial platform for exploring these possibilities and accelerating the industry’s journey towards a sustainable future. Stay tuned for more updates on this exciting collaboration!\n\nCompeting Interest: Dr Pranay Seshadri is an Associate Professor in the Daniel Guggenheim School of Aerospace Engineering in the Georgia Institute of Technology.\nKeywords: Aviation; Artificial Intelligence; Net-Zero; Workshop\n\nThis is the blog for Data-Centric Engineering, an open-access journal published by Cambridge University Press and supported by the Lloyd’s Register Foundation. You can also find us on LinkedIn and X. Here are instructions for submitting an article to the journal."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data-Centric Engineering Blog",
    "section": "",
    "text": "The views expressed in this blog are those of the listed author and do not represent the formal view of the DCE team.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Spectrum of Physics-informed Gaussian Processes for Engineering\n\n\n\nAcademic\n\nEditorial\n\nPaper summary\n\n\n\nProf. Elizabeth Cross and Dr. Matthew Jones from the University of Sheffield discuss Gaussian process regression for physics-informed ML\n\n\n\n\n\nSep 15, 2025\n\n\nElizabeth Cross, Matthew Jones\n\n\n\n\n\n\n\n\n\n\n\n\nTowards Net Zero Aviation with AI\n\n\n\nAcademic\n\n\n\nDr Pranay Seshadri explores opportunities presented by the use of AI within the aviation industry.\n\n\n\n\n\nJun 4, 2024\n\n\nPranay Seshadri\n\n\n\n\n\n\n\n\n\n\n\n\nWhy the Built Environment Needs Innovations in Data-Centric Engineering\n\n\n\nAcademic\n\nTranslational\n\n\n\nDr Ramaseshan Kannan discusses how, despite first appearances, the built environment requires Data-Centric Engineering.\n\n\n\n\n\nApr 9, 2024\n\n\nRamaseshan Kannan\n\n\n\n\n\n\n\n\n\n\n\n\nEngineering, Emulators, Digital Twins and Performance Engineering\n\n\n\nAcademic\n\nSeminar\n\n\n\nProf. Ron Kenett discusses his recent DCE webinar, and how statistical tools can be used to build Digital Twins.\n\n\n\n\n\nFeb 19, 2024\n\n\nRon Kenett\n\n\n\n\n\n\n\n\n\n\n\n\nEditor’s Insights: Data-Centric Engineering of Monitored Systems\n\n\n\nEditorial\n\nAcademic\n\n\n\nProf. Eleni Chatzi (ETH) discusses emerging themes of physics-informed ML, DTs, and systems thinking.\n\n\n\n\n\nNov 28, 2023\n\n\nEleni Chatzi\n\n\n\n\n\n\n\n\n\n\n\n\nThe World Avatar: Revolutionising Human-in-the-Loop\n\n\n\nAcademic\n\n\n\nProf. Markus Kraft discusses work towards human-in-the-loop modelling at scale.\n\n\n\n\n\nNov 13, 2023\n\n\nMarkus Kraft\n\n\n\n\n\n\n\n\n\n\n\n\nSharing Benchmarks to Improve Maritime Safety\n\n\n\nTranslational\n\n\n\nAdrian Clifton (HiLo) explains why data sharing in the maritime industry is critical.\n\n\n\n\n\nOct 4, 2023\n\n\nAdrian Clifton\n\n\n\n\n\n\n\n\n\n\n\n\nManaging Fleets Dynamically Will Be Possible, Thanks to Digitalisation\n\n\n\nTranslational\n\n\n\nAdolfo Crespo del Castillo discusses the digitalisation of rail fleet asset management.\n\n\n\n\n\nFeb 2, 2023\n\n\nAdolfo Crespo del Castillo\n\n\n\n\n\n\n\n\n\n\n\n\nCharacterising Extreme Environments\n\n\n\nAcademic\n\nTranslational\n\n\n\nMatthew Jones and Philip Jonathan (Shell, Lancaster University) introduce ideas from extreme value statistics in weather modelling and oceanography.\n\n\n\n\n\nNov 25, 2022\n\n\nMatthew Jones, Philip Jonathan\n\n\n\n\n\n\n\n\n\n\n\n\nAutomating Supply Chains: How Heavy is My Shopping?\n\n\n\nTranslational\n\n\n\nAdrià Salvador and Ponç Palau (Glovo) present the development of Glovo’s language modelling tools.\n\n\n\n\n\nOct 12, 2022\n\n\nAdrià Salvador, Ponç Palau\n\n\n\n\n\n\n\n\n\n\n\n\nWorking Together for Safer Seas\n\n\n\nTranslational\n\n\n\nAdrian Clifton (HiLo) explains why data sharing in the maritime industry is critical.\n\n\n\n\n\nJun 22, 2022\n\n\nAdrian Clifton\n\n\n\n\n\n\n\n\n\n\n\n\nRemote Sensing for Methane: Monitoring and Reducing Greenhouse Gas Emission\n\n\n\nTranslational\n\nAcademic\n\n\n\nMatthew Jones and Philip Jonathan (Shell, Lancaster University) discuss the importance of monitoring methane to reduce global warming.\n\n\n\n\n\nApr 27, 2022\n\n\nMatthew Jones, Philip Jonathan\n\n\n\n\n\n\n\n\n\n\n\n\nMaterials Informatics: Urgently Needed for a Sustainable Future\n\n\n\nTranslational\n\nPaper summary\n\n\n\nHannah Melia (Citrine Informatics) discusses the growing field of Materials Informatics.\n\n\n\n\n\nDec 15, 2021\n\n\nHannah Melia\n\n\n\n\n\n\n\n\n\n\n\n\nThe DCE Blog: Instructions for Authors\n\n\n\nGuidelines\n\n\n\nThese guidelines are based on those provided by the Data & Policy Blog\n\n\n\n\n\nDec 13, 2021\n\n\nDCE\n\n\n\n\n\nNo matching items"
  }
]